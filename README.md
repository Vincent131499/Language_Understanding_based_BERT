# Language_Understanding_based_BERT
基于BERT的预训练语言模型实现，分为两步：预训练和微调。目前已包括BERT、Roberta、ALbert三个模型，且皆可支持Whole Word Mask模式。

## 项目驱动
本着应用于工业生产的需要，想集成目前业界先进的预训练语言模型，并提供预训练和微调方法以此为用户提供一个端到端的预训练+微调框架。<br>
===细节将于近日更新===
